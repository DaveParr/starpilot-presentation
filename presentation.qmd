---
title: "Presentation"
format:
  revealjs:
    theme: solarized
    transition: zoom
    mermaid: 
      theme: neutral
---

# Introduction

## Goals for the talk

-   Share an AI project I've been working on
-   Share some of the design decisions I made to build it
-   Share some of the experience I had building it
-   Share some of the code I wrote to build it

## Level Set {auto-animate="true"}

ðŸ™Œ ðŸ™ŒðŸ» ðŸ™ŒðŸ¼ ðŸ™ŒðŸ½ ðŸ™ŒðŸ¾ ðŸ™ŒðŸ¿ ðŸ™ŒðŸ¿

::: {.incremental data-id="level-set"}
-   Retrieval Augmented Generation
-   Vector Embedding
-   Vector Store
-   Vector Similarity
:::

::: notes
-   I'm going to assume you know the meaning of terms like 'AI', 'LLM', 'GPT'
-   If you don't, hello, welcome, you are in a great place to learn, but I won't go over them now. Feel free to make a neighbor into friend and ask them, or get Joe to explain it in his session.
-   Can you define these terms?
-   I'm not going to put anyone on the spot here, but I'd love to have people raise their hands if they feel they can define these terms
-   This will help me understand the level of knowledge in the room and tune the rest of the talk
:::

## Level Set {auto-animate="true"}

::: {data-id="level-set"}
-   Retrieval Augmented Generation
    -   Using a language model supported by a vector store
-   Vector Embedding
    -   Converting text into a list of numbers to represent semantic meaning
-   Vector Store
    -   Database optimized for storing embedding vectors
-   Vector Similarity
    -   Quantifying the similarity between embedding vectors
:::

::: notes
-   So here are the definitions of the terms, keep the hands up if you got most of them right
-   If you knew these terms already, great! This talk is largely about applying these concepts in code.
-   If you didn't, that's also great, because you're in the right place to learn about them.
:::

## Level Set {auto-animate="true"}

ðŸ™Œ ðŸ™ŒðŸ» ðŸ™ŒðŸ¼ ðŸ™ŒðŸ½ ðŸ™ŒðŸ¾ ðŸ™ŒðŸ¿ ðŸ™ŒðŸ¿

> Who is comfortable reading `python`?

# The Project

## Goals for an AI project

::: incremental
-   As a Data Scientist
-   I want to build something novel with AI
-   So that I can learn and experiment with the technology
-   And maybe make something useful for other people
:::

::: notes
-   Apply Retrieval Augmented Generation to a goal I have (and maybe other people)
-   Understand how LangChain (and friends) works through building something novel
-   Experiment in a way that leans into what LLMs + AI are good at
    -   Semantic meaning
:::

## What is starpilot?

-   Use you an AI for great repo suggestions
-   Generate personal suggestions of useful packages/tools for your current programming goals


## 

<script src="https://asciinema.org/a/661841.js" id="asciicast-661841" async="true"></script>

::: notes
- So here is a smash cut to the most basic demo.
- I call starpilot to execute the 'shoot' command with the query "machine learning"
- Starpilot returns 10 repos that are all semantically similar to the query, and all exist, no hallucinations here
- Most interestingly the repo `DeepLearningExamples` is returned, which is depending on your level of pedanticness, not a 'machine learning' repo, but a 'deep learning' repo
- However, `DeepLearningExamples` never states the term machine learning. It's a deep learning, and that is semantically similar to machine learning. This is exactly the kind of result I was hoping for. The tool is looking at the search _intention_ not the search _terms_
:::

## Why is 'semantic similarity' important?

-   Keyword search tests "how close a string is to another string"
    -   Exact Match: `cool cats != chill felines`
    -   Partial Match: `cool cat` partially fits `cooler cats`
-   Semantic search tests "how similar in *meaning* is the phrase to another phrase"
    -   `cool cats` is equivalent in meaning to `chill felines`, also `groovy kittys`, `suave main coon` and `sphinx with style`

::: notes
-   I think applying AI, LLM and RAG to a problem like might be a good fit
-   This is because the problem is about semantic similarity
-   I'm looking for a library that does "DataFrames". Will I also be interested in libraries that do "Data Tables"? Probably.
-   I'm looking for a 'Front end' framework, will I also be interested in 'UI' frameworks? Probably.
:::

## Why is this data suitable for 'semantic' but not 'keyword' search?

-   GitHub stars are public and common across languages/stacks/specialisms
-   GitHub repos are text rich data
-   GitHub repos are a 'weak standard'
    -   Topics are free text
    -   Descriptions are free text

::: notes
-   Without access to suitable data, this project can't get off the ground, but luckily GitHub is a treasure trove of easily acciessble data for this use case
-   The data is text rich, and the text is 'rich' in meaning
-   However, the data is also 'varied' in structure, quality and context
-   Repos vary subjectively by community, project requirements, and developer style
-   There is no 'repo police' saying a repo must have a certain structure, or a certain set of tags
-   This is where the AI comes in
:::

# An Intuitive Example

## Polars vs Pandas {auto-animate="true"}

::: columns
::: {.column width="50%" data-id="pandas"}
-   data-analysis
-   flexible
-   alignment
-   python
-   data-science
:::

::: {.column width="50%" data-id="polars"}
-   dataframe-library
-   dataframe
-   dataframes
-   arrow
-   python
-   out-of-core
:::
:::

::: notes
-   To illustrate this point, let's look at a few tags from GitHub
-   This is the topic list for the `Polars` and `Pandas` repos
-   They are functionally extremely close. They are both Python libraries for data manipulation
-   While there may be some subjective views about when to use which they both solve practically identical problems and fit practically identical use-cases
-   If we were to search for 'DataFrames for Python' we would want to return both of these repos
-   However, keyword search would only return one of them because none of the topics are shared
:::


## Polars vs Pandas {auto-animate="true"}

::: columns
::: {.column width="50%" data-id="pandas"}
> Flexible and powerful data analysis / manipulation library for Python, providing labeled data structures similar to R data.frame objects, statistical functions, and much more
:::

::: {.column width="50%" data-id="polars"}
> Dataframes powered by a multithreaded, vectorized query engine, written in Rust
:::
:::

::: notes
-   Here are the descriptions for the `Polars` and `Pandas` repos
-   Again very similar, but keyword search would not return both of these repos
-   I'd suggest unless you knew already what the difference between these two libraries was, you'd be hard pressed to know which one has been which in the last 2 slides
:::

## Polars vs Pandas {auto-animate="true"}

:::: columns
::: {.column width="50%" data-id="pandas"}
![](images/panda4.png)
:::

::: {.column width="50%" data-id="polars"}
![](images/polars3.png)
:::
::::

::: notes
-   If you managed to guess correctly, congratulations, you've won a smug sense of self-satisfaction
-   Even if you didn't, that's fine, because the point is that these two libraries are very similar in meaning, but very different in the way they are described
-   Also, I didn't set this up, but I lobe the way that the panda was generated with 2 laptops for some reason, and the polar bear is sort of almost a pandas
-   It's like the polar bear is all "Look, I'm Pandas too!" and the Panda is furiously dual wielding laptops to attempt to prove it's multi-threaded
:::

## Embedding Example

-  "polars module"
-  "pandas package"
-  "panda breeding"
-  "polar environment"

::: notes
To create an embedding, the source data is converted into a list of numbers through a process called 'embedding'
The numbers represent the semantic meaning of the text
You can imagine each number as a 'relevance score' for a given topic
I'm deliberately simplifying this, but it's a good enough mental model for now
Suppose we have the following 4 pieces of text we want to embed separately
:::

## Embedding Example {auto-animate="true"}

```{mermaid}
flowchart LR

subgraph strings
pm("polars module")
pp("pandas package")
pb("panda breeding")
pe("polar environment")
end

subgraph embedder
embed([function])

pm --> embed
pp --> embed
pb --> embed
pe --> embed
end

subgraph vector
pmv("0.24, 0.12")
ppv("0.91, 0.83")
pbv("0.83, 0.12")
pev("0.91, 0.24")

embed --> pmv
embed --> ppv
embed --> pbv
embed --> pev
end
```

::: notes
The embedding process takes the text and converts it into a list of numbers
In our silly example, the numbers are 2D, but in reality, they are billions of dimensions
:::

## Embedding Example {auto-animate="true"}


```{r}
library(ggplot2)

df <- data.frame(
  query = c("polars module", "pandas package", "panda breeding", "polar environment"),
  relevance_to_zoologists = c(0.24, 0.12, 0.83, 0.70),
  relevance_to_data_scientists = c(0.91, 0.83, 0.24, 0.12),
  stringsAsFactors = FALSE
)

ggplot(df, aes(x=relevance_to_zoologists, y=relevance_to_data_scientists)) +
    geom_point() +
    geom_text(aes(label=query), hjust=0.5, vjust=-0.5, check_overlap = TRUE) +
    scale_x_continuous(limits = c(0, 1)) +
    scale_y_continuous(limits = c(0, 1)) +
    xlab("Relevance to Zoologists") +
    ylab("Relevance to Data Scientists") +
    coord_fixed(ratio = 1)
```

::: notes
Here's a plot of the 4 pieces of text in 2D space
-   The x-axis represents the relevance to zoologists
-   The y-axis represents the relevance to data scientists
-   The points are the 4 pieces of text converted into an embedding
:::

## Embedding Example {auto-animate="true"}

```{r}
df <- rbind(df, data.frame(query = "pandas for data science", relevance_to_zoologists = 0.2, relevance_to_data_scientists = 0.7))

df$distance <- sqrt((df$relevance_to_zoologists - 0.2)^2 + (df$relevance_to_data_scientists - 0.7)^2)

ggplot(df, aes(x=relevance_to_zoologists, y=relevance_to_data_scientists)) +
    geom_point() +
    geom_text(aes(label=query), hjust=0.5, vjust=-0.5, check_overlap = TRUE) +
    geom_text(aes(label=round(distance, 2)), hjust=0.5, vjust=1.5, check_overlap = TRUE) +
    geom_segment(aes(xend = 0.2, yend = 0.7), linetype = "dashed", color = "red") +
    scale_x_continuous(limits = c(0, 1)) +
    scale_y_continuous(limits = c(0, 1)) +
    xlab("Relevance to Zoologists") +
    ylab("Relevance to Data Scientists") +
    coord_fixed(ratio = 1)
```

::: notes
So now lets invent a query to put in the embedding space: "pandas for machine learning"
When we embed this query, it will be placed in the embedding space
From this known point we then compute the distance to the other points
:::

# First Experiment

## Solution Requirements

-   A method to extract the data from GitHub
-   A connection to an llm
-   A framework to parse 100s (1000s?) of 'similarly' structured data
-   A data store for processed data
-   An interactive interface for the developer to access the data

## Solution Architecture

-   PyGithub
    -   GitHub Rest API access
-   Langchain
    -   LLM Rest API Access
    -   Document parsing tools
    -   Vector store prompt query framework
-   Chroma
    -   On disk vector store
-   Typer
    -   'FastAPI' style CLI framework

## Similarity Search

```{mermaid}
flowchart LR

subgraph 0 Resources
	GH([GitHub Repo])
	DB[(Chroma)]
	LLM([LLM])
end

subgraph 1 ETL
	raw(raw_GH/*.json)
	docs_in(Documents)
	vec(Vector Embedding)

	GH--->|PyGitHub| raw
	raw-->|LangChain|docs_in
	docs_in-->|LangChain|LLM
	LLM-->|LangChain|vec
	vec-->|LangChain|DB
end

subgraph 2 Execution
	cli([cli])
	q(query)
	LLM([LLM])
	q_vec(Query Vector)
	docs_out(Document)
	a(suggestions)

	cli-->|Typer|q
	q -->|LangChain|LLM
	LLM -->|LangChain|q_vec
	q_vec -->|LangChain|DB
	DB-->|LangChain|docs_out
	docs_out-->|Typer|a
end
```

## Outcome

-   IT'S ALIVE
    -   Data gets read
    -   Read data gets embedded
    -   Embedded data gets stored
    -   Embedded data gets queried
-   IT STINKS
    -   Reading data is punishingly long
        -   200 stars in 30 minutes
    -   Querying data is basic
        -   Semantic search works, but is broad
        -   'DataFrames for Python' returns Pandas, and Tidyverse in R, and DataTables in JavaScript

# Refinement

## Refined Version

```{mermaid}
flowchart LR

LLM([LLM])
GH([GitHub Repo])
raw(raw_GH/*.json)
prepped(enhanced_GH/*.json)
style prepped fill:red
docs_in(Documents)
vec_doc(Vector Document)
DB[(Chroma)]
self_query(Self Query)
vec_q(Vector Embedding)
db_filter(Filter Value)
cli([cli])
q(query)
docs_out(Document)
a(suggestions)

subgraph 0 Resources
	GH
	DB
	LLM
end

subgraph 1 ETL
	GH--->|GraphQL| raw
	raw-->|LangChain|prepped
	prepped-->|LangChain|docs_in
	docs_in-->|LangChain|LLM
	LLM-->|LangChain|vec_doc
	vec_doc-->DB
end

subgraph 2 Execution
	LLM-->|LangChain|self_query
	self_query-->|LangChain|db_filter
	style self_query fill:red
	self_query-->|LangChain|vec_q
	db_filter-->DB
	style db_filter fill:red
	vec_q --> DB
	cli-->|typer|q
	q -->|LangChain|LLM
	DB-->|LangChain|docs_out
	docs_out-->|Typer|a
end
```

## What this gets us

-   Faster data read with `GraphQL`
    -   Reduces network overhead of a per repo call
        -   Returns are now paginated `json` files
    -   20x speed up
-   Enhanced data preprocessing
    -   Extracts more data from the repo
        -   Repo tags
        -   Stars
        -   Primary Language
    -   More selective on data load
        -   Only load 'popular' repos
        -   Only load 'relevant' repos
-   Enhanced data preprocessing + Self Querying
    -   Enables more nuanced queries like "Dataframes for Python" -\> "Pandas"

::: notes
-   So lets look at some key parts of the code now
:::

# Example Data

## Example JSON in

``` json
{
    "name": "langchain",
    "nameWithOwner": "langchain-ai/langchain",
    "url": "https://github.com/langchain-ai/langchain",
    "homepageUrl": "https://python.langchain.com",
    "description": "\ud83e\udd9c\ud83d\udd17 Build context-aware reasoning applications",
    "stargazerCount": 77908,
    "primaryLanguage": "Python",
    "languages": [
        "Python",
        "Makefile",
        "HTML",
        "Dockerfile",
        "TeX",
        "JavaScript",
        "Shell",
        "XSLT",
        "Jupyter Notebook",
        "MDX"
    ],
    "owner": "langchain-ai",
    "content": "langchain \ud83e\udd9c\ud83d\udd17 Build context-aware reasoning applications Python"
}
```

::: notes
-   This is an example of the json file that is written to disk
-   It contains the metadata and content of a repo
-   Content is created by starpilot by appending the `description` and `name` fields
-   At this stage I have one json per starred repo, and these now need to be processed and loaded into the vector store
-   To do this there is a `prepare_documents` function in Starpilot
:::

# The `prepare_documents` function

## `prepare_documents` {auto-animate="true"}

-   The `prepare_documents` function is responsible for reading in the json files and creating `Document` objects

``` {.python .python}
def prepare_documents(
    repo_contents_dir: str = "./repo_content",
) -> List[Document]:
    """
    Prepare the documents for ingestion into the vectorstore
    """
    ...
    return documents
```

::: notes
-   Here's the function signature
-   It takes a directory of json files as input
-   It returns a list of `Document` objects
-   A `Document` is a class defined in the `LangChain` library
-   The `Document` object contains the content *and* metadata for a given repo
:::

## `prepare_documents` {auto-animate="true"}

Get the file paths for each `json` file for each repo

``` {.python code-line-numbers="7-10"}
def prepare_documents(
    repo_contents_dir: str = "./repo_content",
) -> List[Document]:
    """
    Prepare the documents for ingestion into the vectorstore
    """

    file_paths = []
    for file in os.listdir(repo_contents_dir):
        file_paths.append(os.path.join(repo_contents_dir, file))

    ...

    return documents
```

::: notes
-   `prepare_documents` first creates a list of file paths to the json files
-   Each item in this list is the direct path to a specific json file, each of which is the content of a specific repo read through `GraphQL`
:::

## `prepare_documents` {auto-animate="true"}

Load the `Document` objects from the json files

``` {.python code-line-numbers="5-16"}
def prepare_documents(
    repo_contents_dir: str = "./repo_content",
) -> List[Document]:
    ...
    documents = []
    for file_path in track(file_paths, description="Loading documents..."):
        logger.debug("Loading document", file=file_path)
        loader = JSONLoader(
            file_path,
            jq_schema=".",
            content_key="content",
            metadata_func=_metadata_func,
            text_content=False,
        )
        if (loaded_document := loader.load())[0].page_content != "":
            documents.extend(loaded_document)
    ...
    return documents
```

::: notes
-   The function reads in the json files written per repo into memory
-   The `JSONLoader` class is used to load the json files
-   The `Document` objects are then returned as a list
-   `jq_schema` defines the "root" of the json file
-   `content_key` defines the key in the json file that contains the content
-   `metadata_func` is a function that extracts metadata from the json file
-   Finally if the `Document` object has content it is added to the list of `Document` objects
:::

## `prepare_documents` {auto-animate="true"}

The `metadata_func` function is used to extract metadata from the json file

``` {.python code-line-numbers="12"}
def prepare_documents(
    repo_contents_dir: str = "./repo_content",
) -> List[Document]:
    ...
    documents = []
    for file_path in track(file_paths, description="Loading documents..."):
        logger.debug("Loading document", file=file_path)
        loader = JSONLoader(
            file_path,
            jq_schema=".",
            content_key="content",
            metadata_func=_metadata_func,
            text_content=False,
        )
        if (loaded_document := loader.load())[0].page_content != "":
            documents.extend(loaded_document)
    ...
    return documents
```

::: notes
-   However, what's the `_metadata_func` function?
:::

## `prepare_documents` {auto-animate="true"}

The `_metadata_func` function is used to extract metadata from the json file

``` {.python code-line-numbers="5-18"}
def prepare_documents(
    repo_contents_dir: str = "./repo_content",
) -> List[Document]:

    def _metadata_func(record: dict, metadata: dict) -> dict:
        metadata["url"] = record.get("url")
        metadata["name"] = record.get("name")
        metadata["stargazerCount"] = record["stargazerCount"]
        if (primary_language := record.get("primaryLanguage")) is not None:
            metadata["primaryLanguage"] = primary_language
        if (description := record.get("description")) is not None:
            metadata["description"] = description
        if (topics := record.get("topics")) is not None:
            metadata["topics"] = " ".join(topics)
        if (languages := record.get("languages")) is not None:
            metadata["languages"] = " ".join(languages)

        return metadata

    ...

    return documents
```

::: notes
-   `_metadata_func` is a function that extracts metadata for the repo and processes it to be included as the metadata of the `Document` object
-   It extracts the `url`, `name`, `stargazerCount`, `primaryLanguage`, `description`, `topics` and `languages` fields from the json file as long as they exist
-   The items that are extracted are then added to the `metadata` dictionary
-   The `metadata` dictionary is then returned as part of the `Document` object
:::

## Using the `prepare_documents` function {auto-animate="true"}

`prepare_documents` is used to load the `Document` objects into \`Chroma

``` {.python code-line-numbers="5"}
from langchain_community.vectorstores import Chroma
from langchain_openai.embeddings import OpenAIEmbeddings

Chroma.from_documents(
    documents=utils.prepare_documents(),
    embedding=OpenAIEmbeddings(model="text-embedding-3-large"),
    persist_directory="./vectorstore-chroma",
)
```

::: notes
-   To use this list of `Documents` , we need to pass it to the `Chroma` class using the `from_documents` method.
:::

## Using the `prepare_documents` function {auto-animate="true"}

`Chroma.from_documents` also needs an `embedding` object and a `persist_directory`

``` {.python code-line-numbers="6-7"}
from langchain_community.vectorstores import Chroma
from langchain_openai.embeddings import OpenAIEmbeddings

Chroma.from_documents(
    documents=utils.prepare_documents(),
    embedding=OpenAIEmbeddings(model="text-embedding-3-large"),
    persist_directory="./vectorstore-chroma",
)
```

::: notes
-   The `Chroma` class is a class that is part of the `LangChain` library
-   We also need to pass an `embedding` object to the `Chroma` class
-   On call, this method will create a vector store from the `Document` objects and then for each `Document` object, it will embed the content of the `Document` object using the `OpenAIEmbeddings` class and store the embeddings in the vector store
-   The metadata of the `Document` object is also stored in the vector store, but not embedded. We'll see why this is important later
:::

# Accessing the Vector Store

## Semantic similarity search {auto-animate="true"}

Semantic search accepts a query string and returns the most relevant `Document` objects

``` {.python code-line-numbers="5"}
def shoot(
    vectorstore_path: str,
    k: int,
    method: SearchMethods = SearchMethods.similarity,
    query: str,
) -> List[Document]:
    """
    Create a retriever from a vectorstore and query it
    """
```

::: notes
-   This is the function signature for the `create_retriever` function in starpilot
:::

## Semantic similarity search {auto-animate="true"}

Create a `retriever` object from the vector store and query it

``` {.python code-line-numbers="9-21"}
def shoot(
        vectorstore_path: str,
    k: int,
    method: SearchMethods = SearchMethods.similarity,
    query: str,
) -> List[Document]:
    """
    Create a retriever from a vectorstore and query it
    """
    retriever = Chroma(
        persist_directory=vectorstore_path,
        embedding_function=OpenAIEmbeddings(
            model="text-embedding-3-large"
        ),
    ).as_retriever(
        search_type=method,
        search_kwargs={
            "k": k,
        },
    )
    return retriever.get_relevant_documents(query)
```

::: notes
-   This is the `shoot` function in starpilot, which is used to perform a semantic search on the vector store
-   First, we need to create a retriever object from the vector store Class
-   We need to specify the embedding function that was used to embed the content of the `Document` object
-   We also need to specify the search method that we want to use, which is effectively the algorithm to compute the similarity between the query vector and the vectors in the vector store
-   Finally, we need to pass the query string to the `get_relevant_documents` method of the retriever object, and it will return the most relevant `Document` objects
:::

## DEMO SHOOT

<!-- TODO -->

# Self Querying

## Self Querying {auto-animate="true"}

Self-querying is a way to pre-filter the results of a semantic search by metadata fields

```` {.python code-line-numbers="1-16"}
def astrologer(
    query: str,
    k: Optional[int] = typer.Option(
        4, help="Number of results to fetch from the vectorstore"
    ),
) -> List[Document]:
    """
    A self-query of the vectorstore that allows the user to search for a repo while filtering by attributes

    Example:
    ```
    starpilot astrologer "What can I use to build a web app with Python?"
    starpilot astrologer "Suggest some Rust machine learning crates"
    ```

    """
    metadata_field_info = [
        AttributeInfo(
            name="languages",
            description="the programming languages of a repo. Example: ['python', 'R', 'Rust']",
            type="string",
        ),
        AttributeInfo(
            name="name",
            description="the name of a repository. Example: 'langchain'",
            type="string",
        ),
        AttributeInfo(
            name="topics",
            description="the topics a repository is tagged with. Example: ['data-science', 'machine-learning', 'web-development', 'tidyverse']",
            type="string",
        ),
        AttributeInfo(
            name="url",
            description="the url of a repository on GitHub",
            type="string",
        ),
        AttributeInfo(
            name="stargazerCount",
            description="the number of stars a repository has on GitHub",
            type="number",
        ),
    ]

    document_content_description = "content describing a repository on GitHub"

    prompt = get_query_constructor_prompt(
        document_content_description,
        metadata_field_info,
        examples=[
            (
                "Python machine learning repos",
                {
                    "query": "machine learning",
                    "filter": 'eq("primaryLanguage", "Python")',
                },
            ),
            (
                "Rust Dataframe crates",
                {"query": "data frame", "filter": 'eq("primaryLanguage", "Rust")'},
            ),
            (
                "What R packages do time series analysis",
                {"query": "time series", "filter": 'eq("primaryLanguage", "R")'},
            ),
            (
                "data frame packages with 100 stars or more",
                {
                    "query": "data frame",
                    "filter": 'gte("stargazerCount", 100)',
                },
            ),
        ],
        allowed_comparators=[
            Comparator.EQ,
            Comparator.NE,
            Comparator.GT,
            Comparator.GTE,
            Comparator.LT,
            Comparator.LTE,
        ],
    )

    llm = ChatOpenAI(model="gpt-3.5-turbo",)

    output_parser = StructuredQueryOutputParser.from_components()

    query_constructor = prompt | llm | output_parser

    vectorstore = Chroma(
        persist_directory="./vectorstore-chroma",
        embedding_function=OpenAIEmbeddings(model="text-embedding-3-large"),
    )

    retriever = SelfQueryRetriever(
        query_constructor=query_constructor,
        vectorstore=vectorstore,
        structured_query_translator=ChromaTranslator(),
        search_kwargs={"k": k},
    )

    results = retriever.invoke(query)

    return results
````

::: notes
-   So this is the `astrologer` function in starpilot
-   It's distinct from the `shoot` function in that it allows the user to filter the results by metadata fields
-   To do this we use more LangChain classes to orchestrate the query
:::

## Self Querying {auto-animate="true"}

Create a list of `AttributeInfo` objects that describe the metadata fields

```` {.python code-line-numbers="17-43"}
def astrologer(
    query: str,
    k: Optional[int] = typer.Option(
        4, help="Number of results to fetch from the vectorstore"
    ),
) -> List[Document]:
    """
    A self-query of the vectorstore that allows the user to search for a repo while filtering by attributes

    Example:
    ```
    starpilot astrologer "What can I use to build a web app with Python?"
    starpilot astrologer "Suggest some Rust machine learning crates"
    ```

    """
    metadata_field_info = [
        AttributeInfo(
            name="languages",
            description="the programming languages of a repo. Example: ['python', 'R', 'Rust']",
            type="string",
        ),
        AttributeInfo(
            name="name",
            description="the name of a repository. Example: 'langchain'",
            type="string",
        ),
        AttributeInfo(
            name="topics",
            description="the topics a repository is tagged with. Example: ['data-science', 'machine-learning', 'web-development', 'tidyverse']",
            type="string",
        ),
        AttributeInfo(
            name="url",
            description="the url of a repository on GitHub",
            type="string",
        ),
        AttributeInfo(
            name="stargazerCount",
            description="the number of stars a repository has on GitHub",
            type="number",
        ),
    ]

    document_content_description = "content describing a repository on GitHub"

    prompt = get_query_constructor_prompt(
        document_content_description,
        metadata_field_info,
        examples=[
            (
                "Python machine learning repos",
                {
                    "query": "machine learning",
                    "filter": 'eq("primaryLanguage", "Python")',
                },
            ),
            (
                "Rust Dataframe crates",
                {"query": "data frame", "filter": 'eq("primaryLanguage", "Rust")'},
            ),
            (
                "What R packages do time series analysis",
                {"query": "time series", "filter": 'eq("primaryLanguage", "R")'},
            ),
            (
                "data frame packages with 100 stars or more",
                {
                    "query": "data frame",
                    "filter": 'gte("stargazerCount", 100)',
                },
            ),
        ],
        allowed_comparators=[
            Comparator.EQ,
            Comparator.NE,
            Comparator.GT,
            Comparator.GTE,
            Comparator.LT,
            Comparator.LTE,
        ],
    )

    llm = ChatOpenAI(model="gpt-3.5-turbo",)

    output_parser = StructuredQueryOutputParser.from_components()

    query_constructor = prompt | llm | output_parser

    vectorstore = Chroma(
        persist_directory="./vectorstore-chroma",
        embedding_function=OpenAIEmbeddings(model="text-embedding-3-large"),
    )

    retriever = SelfQueryRetriever(
        query_constructor=query_constructor,
        vectorstore=vectorstore,
        structured_query_translator=ChromaTranslator(),
        search_kwargs={"k": k},
    )

    results = retriever.invoke(query)

    return results
````

::: notes
-   To enable self-querying, we are effectively creating a prompt for an llm
-   The prompt is going to receive the users query, then it will destructure the query into the part that is the query and the part that is the filter
-   To do this we need to describe those metadata fields from earlier we want to filter by
-   LangChain provides `AttributeInfo` objects to describe the metadata fields
-   We provide one for each field and collect them into a list
:::

## Self Querying {auto-animate="true"}

Construct the query prompt using get query constructor prompt to return a `BasePromptTemplate` object

```` {.python code-line-numbers="45-82"}
def astrologer(
    query: str,
    k: Optional[int] = typer.Option(
        4, help="Number of results to fetch from the vectorstore"
    ),
) -> List[Document]:
    """
    A self-query of the vectorstore that allows the user to search for a repo while filtering by attributes

    Example:
    ```
    starpilot astrologer "What can I use to build a web app with Python?"
    starpilot astrologer "Suggest some Rust machine learning crates"
    ```

    """
    metadata_field_info = [
        AttributeInfo(
            name="languages",
            description="the programming languages of a repo. Example: ['python', 'R', 'Rust']",
            type="string",
        ),
        AttributeInfo(
            name="name",
            description="the name of a repository. Example: 'langchain'",
            type="string",
        ),
        AttributeInfo(
            name="topics",
            description="the topics a repository is tagged with. Example: ['data-science', 'machine-learning', 'web-development', 'tidyverse']",
            type="string",
        ),
        AttributeInfo(
            name="url",
            description="the url of a repository on GitHub",
            type="string",
        ),
        AttributeInfo(
            name="stargazerCount",
            description="the number of stars a repository has on GitHub",
            type="number",
        ),
    ]

    document_content_description = "content describing a repository on GitHub"

    prompt = get_query_constructor_prompt(
        document_content_description,
        metadata_field_info,
        examples=[
            (
                "Python machine learning repos",
                {
                    "query": "machine learning",
                    "filter": 'eq("primaryLanguage", "Python")',
                },
            ),
            (
                "Rust Dataframe crates",
                {"query": "data frame", "filter": 'eq("primaryLanguage", "Rust")'},
            ),
            (
                "What R packages do time series analysis",
                {"query": "time series", "filter": 'eq("primaryLanguage", "R")'},
            ),
            (
                "data frame packages with 100 stars or more",
                {
                    "query": "data frame",
                    "filter": 'gte("stargazerCount", 100)',
                },
            ),
        ],
        allowed_comparators=[
            Comparator.EQ,
            Comparator.NE,
            Comparator.GT,
            Comparator.GTE,
            Comparator.LT,
            Comparator.LTE,
        ],
    )

    llm = ChatOpenAI(model="gpt-3.5-turbo",)

    output_parser = StructuredQueryOutputParser.from_components()

    query_constructor = prompt | llm | output_parser

    vectorstore = Chroma(
        persist_directory="./vectorstore-chroma",
        embedding_function=OpenAIEmbeddings(model="text-embedding-3-large"),
    )

    retriever = SelfQueryRetriever(
        query_constructor=query_constructor,
        vectorstore=vectorstore,
        structured_query_translator=ChromaTranslator(),
        search_kwargs={"k": k},
    )

    results = retriever.invoke(query)

    return results
````

::: notes
-   As well as the metadata fields, we describe the kind of data we are querying
-   We also provide examples of queries and filters in a 'few-shot' style prompting format
-   The examples should reference the metadata fields we described earlier
-   The `get_query_constructor_prompt` function also accepts a list of allowed comparators, which are specific to the vector store implementation. `Chroma` will have different comparators to `Weaviate` for example
:::

## Self Querying {auto-animate="true"}

Use LangChain Expression Language in a chain to make a `QueryConstructor` object

```` {.python code-line-numbers="84-88"}
def astrologer(
    query: str,
    k: Optional[int] = typer.Option(
        4, help="Number of results to fetch from the vectorstore"
    ),
) -> List[Document]:
    """
    A self-query of the vectorstore that allows the user to search for a repo while filtering by attributes

    Example:
    ```
    starpilot astrologer "What can I use to build a web app with Python?"
    starpilot astrologer "Suggest some Rust machine learning crates"
    ```

    """
    metadata_field_info = [
        AttributeInfo(
            name="languages",
            description="the programming languages of a repo. Example: ['python', 'R', 'Rust']",
            type="string",
        ),
        AttributeInfo(
            name="name",
            description="the name of a repository. Example: 'langchain'",
            type="string",
        ),
        AttributeInfo(
            name="topics",
            description="the topics a repository is tagged with. Example: ['data-science', 'machine-learning', 'web-development', 'tidyverse']",
            type="string",
        ),
        AttributeInfo(
            name="url",
            description="the url of a repository on GitHub",
            type="string",
        ),
        AttributeInfo(
            name="stargazerCount",
            description="the number of stars a repository has on GitHub",
            type="number",
        ),
    ]

    document_content_description = "content describing a repository on GitHub"

    prompt = get_query_constructor_prompt(
        document_content_description,
        metadata_field_info,
        examples=[
            (
                "Python machine learning repos",
                {
                    "query": "machine learning",
                    "filter": 'eq("primaryLanguage", "Python")',
                },
            ),
            (
                "Rust Dataframe crates",
                {"query": "data frame", "filter": 'eq("primaryLanguage", "Rust")'},
            ),
            (
                "What R packages do time series analysis",
                {"query": "time series", "filter": 'eq("primaryLanguage", "R")'},
            ),
            (
                "data frame packages with 100 stars or more",
                {
                    "query": "data frame",
                    "filter": 'gte("stargazerCount", 100)',
                },
            ),
        ],
        allowed_comparators=[
            Comparator.EQ,
            Comparator.NE,
            Comparator.GT,
            Comparator.GTE,
            Comparator.LT,
            Comparator.LTE,
        ],
    )

    llm = ChatOpenAI(model="gpt-3.5-turbo",)

    output_parser = StructuredQueryOutputParser.from_components()

    query_constructor = prompt | llm | output_parser

    vectorstore = Chroma(
        persist_directory="./vectorstore-chroma",
        embedding_function=OpenAIEmbeddings(model="text-embedding-3-large"),
    )

    retriever = SelfQueryRetriever(
        query_constructor=query_constructor,
        vectorstore=vectorstore,
        structured_query_translator=ChromaTranslator(),
        search_kwargs={"k": k},
    )

    results = retriever.invoke(query)

    return results
````

::: notes
-   So LCEL is a chain of functions that are used to create a `QueryConstructor` object
-   LCEL allows you to pipe functions together to return this execution chain
-   Similar to the `|` operator in bash or `.pipe()` in pandas
:::

## Self Querying {auto-animate="true"}

Create a `SelfQueryRetriever` object from the `QueryConstructor` object and the `Chroma` object

```` {.python code-line-numbers="90-104"}
def astrologer(
    query: str,
    k: Optional[int] = typer.Option(
        4, help="Number of results to fetch from the vectorstore"
    ),
) -> List[Document]:
    """
    A self-query of the vectorstore that allows the user to search for a repo while filtering by attributes

    Example:
    ```
    starpilot astrologer "What can I use to build a web app with Python?"
    starpilot astrologer "Suggest some Rust machine learning crates"
    ```

    """
    metadata_field_info = [
        AttributeInfo(
            name="languages",
            description="the programming languages of a repo. Example: ['python', 'R', 'Rust']",
            type="string",
        ),
        AttributeInfo(
            name="name",
            description="the name of a repository. Example: 'langchain'",
            type="string",
        ),
        AttributeInfo(
            name="topics",
            description="the topics a repository is tagged with. Example: ['data-science', 'machine-learning', 'web-development', 'tidyverse']",
            type="string",
        ),
        AttributeInfo(
            name="url",
            description="the url of a repository on GitHub",
            type="string",
        ),
        AttributeInfo(
            name="stargazerCount",
            description="the number of stars a repository has on GitHub",
            type="number",
        ),
    ]

    document_content_description = "content describing a repository on GitHub"

    prompt = get_query_constructor_prompt(
        document_content_description,
        metadata_field_info,
        examples=[
            (
                "Python machine learning repos",
                {
                    "query": "machine learning",
                    "filter": 'eq("primaryLanguage", "Python")',
                },
            ),
            (
                "Rust Dataframe crates",
                {"query": "data frame", "filter": 'eq("primaryLanguage", "Rust")'},
            ),
            (
                "What R packages do time series analysis",
                {"query": "time series", "filter": 'eq("primaryLanguage", "R")'},
            ),
            (
                "data frame packages with 100 stars or more",
                {
                    "query": "data frame",
                    "filter": 'gte("stargazerCount", 100)',
                },
            ),
        ],
        allowed_comparators=[
            Comparator.EQ,
            Comparator.NE,
            Comparator.GT,
            Comparator.GTE,
            Comparator.LT,
            Comparator.LTE,
        ],
    )

    llm = ChatOpenAI(model="gpt-3.5-turbo",)

    output_parser = StructuredQueryOutputParser.from_components()

    query_constructor = prompt | llm | output_parser

    vectorstore = Chroma(
        persist_directory="./vectorstore-chroma",
        embedding_function=OpenAIEmbeddings(model="text-embedding-3-large"),
    )

    retriever = SelfQueryRetriever(
        query_constructor=query_constructor,
        vectorstore=vectorstore,
        structured_query_translator=ChromaTranslator(),
        search_kwargs={"k": k},
    )

    results = retriever.invoke(query)

    return results
````

::: notes
-   Finally we instantiate the `SelfQueryRetriever` class with the `QueryConstructor` object we created earlier and the `Chroma` object like last time
-   We then call the `invoke` method on the `SelfQueryRetriever` object with the query string
-   The `invoke` method will return the most relevant `Document` objects *after* filtering by the metadata fields
:::

## 

<script src="https://asciinema.org/a/UvFTn7EMZoUVC8eMbWU59mNyc.js" id="asciicast-UvFTn7EMZoUVC8eMbWU59mNyc" async="true"></script>

# A sidenote on my experience using LangChain

## What I liked {auto-animate="true"}

-   `Langchain` is an "extensively" documented library
-   The LangChain *company* has worked really hard to produce lots of examples and tutorials
    -   Lance from LangChain has great hands on YT videos
-   LangChain fulfills the goal of 'being the glue between LLM/AI/Vectorstore services'
    -   Kind of like `scikit-learn` for LLMs

## What I didn't like {auto-animate="true"}

-   *heavily* OOP
-   Must read source code to understand exactly what is and isn't supported for each class
    -   Entirely unclear when `kwargs` are actually needed, or when they are silently ignored
    -   'Wrappers' and 'Connectors' don't implement the full API
-   Moves fast and breaks things (\@`v0.1.*`)
    -   Documentation is not always up to date and often conflicting
    -   Huge structural changes (\@`v0.2.*`) split into `langchain` and `langchain_community`
-   Observability is secondary
    -   LangChain is a *company* after all

# Conclusion

## Outcome for developers
-  Available now

## Outcome for me
-  I have a working prototype
-  I have a better understanding of the LangChain library
-  I've translated this understanding into another, similar project for Magic the Gathering underway

## Offer to the audience
-  This is an Open Source project


